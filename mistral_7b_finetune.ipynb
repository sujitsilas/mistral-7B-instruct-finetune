{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct Fine-tuning for Engineering Document Q&A\n",
    "\n",
    "This notebook demonstrates domain adaptation of Mistral-7B-Instruct-v0.3 for engineering document Q&A using LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune\n",
      "Data directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data\n",
      "Model directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models\n",
      "Output directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set base paths\n",
    "BASE_DIR = Path(\"/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune\")\n",
    "FINETUNE_DIR = BASE_DIR / \"mistral-finetune\"\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Convert CSV Q&A data to JSONL format required by mistral-finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 267\n",
      "\n",
      "Columns: ['input_query', 'output_expected_answer', 'pdf_name', 'question_number']\n",
      "\n",
      "First sample:\n",
      "input_query               What is the maximum defrost duration in minute...\n",
      "output_expected_answer    The maximum defrost duration for an Ascend Â® F...\n",
      "pdf_name                                           03_ascend_jhd_series.pdf\n",
      "question_number                                                           1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data\n",
    "csv_path = BASE_DIR / \"rag_eval_QA.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 267 instruction samples\n",
      "\n",
      "Example sample:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a technical assistant specialized in commercial refrigeration equipment. Provide accurate, concise answers based on equipment manuals and documentation.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is the maximum defrost duration in minutes for an Ascend \\u00ae Freezer according to the default settings?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"The maximum defrost duration for an Ascend \\u00ae Freezer is 30 minutes as per the default settings.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Convert to instruction format\n",
    "def create_instruction_sample(row):\n",
    "    \"\"\"\n",
    "    Convert Q&A pair to Mistral instruct format.\n",
    "    Format: user asks question, assistant provides answer.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a technical assistant specialized in commercial refrigeration equipment. \"\n",
    "        \"Provide accurate, concise answers based on equipment manuals and documentation.\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row['input_query']\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": row['output_expected_answer']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Convert all samples\n",
    "samples = [create_instruction_sample(row) for _, row in df.iterrows()]\n",
    "\n",
    "print(f\"Created {len(samples)} instruction samples\")\n",
    "print(f\"\\nExample sample:\")\n",
    "print(json.dumps(samples[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 240\n",
      "Validation samples: 27\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation sets (90/10 split)\n",
    "train_samples, val_samples = train_test_split(\n",
    "    samples, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training data saved to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/train_instruct.jsonl\n",
      "âœ“ Validation data saved to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/val_instruct.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save to JSONL format\n",
    "train_path = DATA_DIR / \"train_instruct.jsonl\"\n",
    "val_path = DATA_DIR / \"val_instruct.jsonl\"\n",
    "\n",
    "# Write training data\n",
    "with open(train_path, 'w') as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "# Write validation data\n",
    "with open(val_path, 'w') as f:\n",
    "    for sample in val_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "print(f\"âœ“ Training data saved to: {train_path}\")\n",
    "print(f\"âœ“ Validation data saved to: {val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Mistral-7B-Instruct-v0.3 Model\n",
    "\n",
    "Download the base model for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar\"\n",
    "model_tar = MODEL_DIR / \"mistral-7B-Instruct-v0.3.tar\"\n",
    "model_extract_dir = MODEL_DIR / \"mistral-7B-Instruct-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mistral-7B-Instruct-v0.3...\n",
      "--2025-12-09 11:47:19--  https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar\n",
      "Resolving models.mistralcdn.com (models.mistralcdn.com)... 172.67.70.68, 104.26.6.117, 104.26.7.117, ...\n",
      "Connecting to models.mistralcdn.com (models.mistralcdn.com)|172.67.70.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14496675840 (14G) [application/x-tar]\n",
      "Saving to: â€˜/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3.tarâ€™\n",
      "\n",
      "/home/scumpia-mrl/D 100%[===================>]  13.50G  21.7MB/s    in 9m 1s   \n",
      "\n",
      "2025-12-09 11:56:21 (25.5 MB/s) - â€˜/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3.tarâ€™ saved [14496675840/14496675840]\n",
      "\n",
      "Extracting model...\n",
      "âœ“ Model extracted to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3\n",
      "\n",
      "Verify checksum with: md5sum /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3.tar\n",
      "Expected: 80b71fcb6416085bcb4efad86dfb4d52\n"
     ]
    }
   ],
   "source": [
    "# Download Mistral-7B-Instruct-v0.3\n",
    "\n",
    "\n",
    "if not model_extract_dir.exists():\n",
    "    print(\"Downloading Mistral-7B-Instruct-v0.3...\")\n",
    "    !wget -O {model_tar} {model_url}\n",
    "    \n",
    "    print(\"Extracting model...\")\n",
    "    !tar -xf {model_tar} -C {MODEL_DIR}\n",
    "    \n",
    "    print(f\"âœ“ Model extracted to: {model_extract_dir}\")\n",
    "else:\n",
    "    print(f\"âœ“ Model already exists at: {model_extract_dir}\")\n",
    "\n",
    "# Verify checksum (optional)\n",
    "expected_checksum = \"80b71fcb6416085bcb4efad86dfb4d52\"\n",
    "print(f\"\\nVerify checksum with: md5sum {model_tar}\")\n",
    "print(f\"Expected: {expected_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration for engineering document Q&A (optimized for single GPU)\n",
    "config = {\n",
    "    # Data paths\n",
    "    \"data\": {\n",
    "        \"instruct_data\": str(train_path),\n",
    "        \"data\": \"\",  # No pretraining data\n",
    "        \"eval_instruct_data\": str(val_path)\n",
    "    },\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model_id_or_path\": str(model_extract_dir),\n",
    "    \"lora\": {\n",
    "        \"rank\": 64  # LoRA rank - can increase to 128 if you have more VRAM\n",
    "    },\n",
    "    \n",
    "    # Training hyperparameters (optimized for single GPU - 31GB VRAM)\n",
    "    \"seq_len\": 4096,  # 4K context (increase to 8192 if you have 40GB+ VRAM)\n",
    "    \"batch_size\": 1,  # Batch size 1 for single GPU\n",
    "    \"max_steps\": 500,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    \"optim\": {\n",
    "        \"lr\": 6e-5,  # Learning rate\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"pct_start\": 0.05\n",
    "    },\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    \"seed\": 42,\n",
    "    \"log_freq\": 10,\n",
    "    \"eval_freq\": 100,\n",
    "    \"no_eval\": False,\n",
    "    \"ckpt_freq\": 100,\n",
    "    \n",
    "    # Save configuration\n",
    "    \"save_adapters\": True,  # Save only LoRA adapters (smaller size)\n",
    "    \"run_dir\": str(OUTPUT_DIR / \"run_001\"),\n",
    "    \n",
    "    # Weights & Biases (optional)\n",
    "    \"wandb\": {\n",
    "        \"project\": \"mistral-7b-engineering-qa\",\n",
    "        \"run_name\": \"engineering-docs-lora-single-gpu\",\n",
    "        \"key\": \"\",  # Add your W&B API key if using wandb\n",
    "        \"offline\": True  # Set to False if using W&B\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = BASE_DIR / \"train_config.yaml\"\n",
    "\n",
    "import yaml\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"âœ“ Configuration saved to: {config_path}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(yaml.dump(config, default_flow_style=False, sort_keys=False))\n",
    "print(f\"\\nâš¡ Optimized for single GPU (31GB VRAM):\")\n",
    "print(f\"  â€¢ 4K context window (memory optimized)\")\n",
    "print(f\"  â€¢ Batch size 1 for single GPU\")\n",
    "print(f\"  â€¢ Expected memory usage: ~20-25GB\")\n",
    "print(f\"\\nðŸ’¡ Tip: Increase seq_len to 8192 or batch_size to 2 if you have more VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch single GPU training (recommended for RTX 5000 Ada or similar)\n",
    "import subprocess\n",
    "\n",
    "# Change to mistral-finetune directory\n",
    "os.chdir(FINETUNE_DIR)\n",
    "\n",
    "# Run training with single GPU\n",
    "cmd = [\n",
    "    \"torchrun\",\n",
    "    \"--nproc_per_node=1\",  # Single GPU\n",
    "    \"--master_port=29500\",\n",
    "    \"train.py\",\n",
    "    str(config_path)\n",
    "]\n",
    "\n",
    "print(\"Starting fine-tuning on single GPU...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run training (this will take 2-3 hours depending on your GPU)\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitor Training\n",
    "\n",
    "Training logs and checkpoints will be saved to the `run_dir` specified in config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training output\n",
    "run_dir = OUTPUT_DIR / \"run_001\"\n",
    "\n",
    "if run_dir.exists():\n",
    "    print(f\"Training artifacts in: {run_dir}\")\n",
    "    print(f\"\\nDirectory contents:\")\n",
    "    !ls -lh {run_dir}\n",
    "else:\n",
    "    print(f\"Training not started yet. Run the training cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference with Fine-tuned Model\n",
    "\n",
    "**Important**: The Mistral official format (`consolidated.safetensors`) requires different tools than standard HuggingFace transformers.\n",
    "\n",
    "You have two options for inference:\n",
    "\n",
    "### Option 1: Use mistral-inference (Recommended - Faster & Less Memory)\n",
    "Use the official Mistral inference library which natively supports the model format and LoRA adapters:\n",
    "\n",
    "```bash\n",
    "pip install mistral-inference\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Works directly with Mistral official format (no conversion needed)\n",
    "- More memory efficient\n",
    "- Faster inference\n",
    "- Native LoRA support\n",
    "\n",
    "See cells below for implementation.\n",
    "\n",
    "### Option 2: Convert to HuggingFace format (More Flexible)\n",
    "Download the HuggingFace version of the model for use with transformers library:\n",
    "\n",
    "**Advantages:**\n",
    "- Compatible with HuggingFace ecosystem\n",
    "- More deployment options\n",
    "- Better integration with other tools\n",
    "\n",
    "You'll need to download the HuggingFace version (~14GB additional download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Inference with mistral-inference (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mistral model from: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3\n",
      "Loading LoRA adapter from: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/output/run_001/checkpoints/checkpoint_000500\n",
      "âœ“ Model loaded successfully with mistral-inference!\n",
      "âœ“ LoRA adapter loaded!\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model with mistral-inference\n",
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Paths\n",
    "mistral_model_path = model_extract_dir  # Original Mistral format model\n",
    "lora_adapter_path = \"/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/output/run_001/checkpoints/checkpoint_000500\"\n",
    "\n",
    "print(f\"Loading Mistral model from: {mistral_model_path}\")\n",
    "print(f\"Loading LoRA adapter from: {lora_adapter_path}\")\n",
    "\n",
    "# Load base model first\n",
    "model = Transformer.from_folder(\n",
    "    str(mistral_model_path),\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Then load the LoRA adapter\n",
    "lora_path = Path(lora_adapter_path) / \"consolidated\" / \"lora.safetensors\"\n",
    "model.load_lora(str(lora_path))\n",
    "\n",
    "print(\"âœ“ Model loaded successfully with mistral-inference!\")\n",
    "print(\"âœ“ LoRA adapter loaded!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fine-tuned model with mistral-inference:\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question 1: What is the maximum defrost duration for an Ascend Freezer?\n",
      "\n",
      "Answer: The maximum defrost duration for an Ascend Freezer is 30 minutes.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question 2: Can I use an extension cord for my Arctic Air Chef Base Refrigerator?\n",
      "\n",
      "Answer: No, the Use of extension cords is strictly prohibited and will also void warranty.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Question 3: What is the compressor part number for an EST-48-N-V model under the Turbo Air E-LINE series?\n",
      "\n",
      "Answer: The compressor part number for an EST-48-N-V model is P0189E1400.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test inference with mistral-inference\n",
    "from mistral_common.protocol.instruct.messages import UserMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_path = mistral_model_path / \"tokenizer.model.v3\"\n",
    "tokenizer = MistralTokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "def generate_answer_mistral(question, max_tokens=512):\n",
    "    \"\"\"Generate answer using mistral-inference.\"\"\"\n",
    "    \n",
    "    # Create chat completion request\n",
    "    messages = [UserMessage(content=question)]\n",
    "    request = ChatCompletionRequest(messages=messages)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode_chat_completion(request).tokens\n",
    "    \n",
    "    # Generate\n",
    "    generated_tokens, _ = generate(\n",
    "        [tokens],\n",
    "        model,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7,\n",
    "        eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    result = tokenizer.instruct_tokenizer.tokenizer.decode(generated_tokens[0])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is the maximum defrost duration for an Ascend Freezer?\",\n",
    "    \"Can I use an extension cord for my Arctic Air Chef Base Refrigerator?\",\n",
    "    \"What is the compressor part number for an EST-48-N-V model under the Turbo Air E-LINE series?\"\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model with mistral-inference:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    answer = generate_answer_mistral(question)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-finetune (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
