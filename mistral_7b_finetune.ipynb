{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-7B-Instruct Fine-tuning for Engineering Document Q&A\n",
    "\n",
    "This notebook demonstrates domain adaptation of Mistral-7B-Instruct-v0.3 for engineering document Q&A using LoRA fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune\n",
      "Data directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data\n",
      "Model directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models\n",
      "Output directory: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set base paths\n",
    "BASE_DIR = Path(\"/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune\")\n",
    "FINETUNE_DIR = BASE_DIR / \"mistral-finetune\"\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODEL_DIR = BASE_DIR / \"models\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Convert CSV Q&A data to JSONL format required by mistral-finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 267\n",
      "\n",
      "Columns: ['input_query', 'output_expected_answer', 'pdf_name', 'question_number']\n",
      "\n",
      "First sample:\n",
      "input_query               What is the maximum defrost duration in minute...\n",
      "output_expected_answer    The maximum defrost duration for an Ascend ® F...\n",
      "pdf_name                                           03_ascend_jhd_series.pdf\n",
      "question_number                                                           1\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data\n",
    "csv_path = BASE_DIR / \"rag_eval_QA.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst sample:\")\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 267 instruction samples\n",
      "\n",
      "Example sample:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"You are a technical assistant specialized in commercial refrigeration equipment. Provide accurate, concise answers based on equipment manuals and documentation.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"What is the maximum defrost duration in minutes for an Ascend \\u00ae Freezer according to the default settings?\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"The maximum defrost duration for an Ascend \\u00ae Freezer is 30 minutes as per the default settings.\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Convert to instruction format\n",
    "def create_instruction_sample(row):\n",
    "    \"\"\"\n",
    "    Convert Q&A pair to Mistral instruct format.\n",
    "    Format: user asks question, assistant provides answer.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a technical assistant specialized in commercial refrigeration equipment. \"\n",
    "        \"Provide accurate, concise answers based on equipment manuals and documentation.\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": row['input_query']\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": row['output_expected_answer']\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Convert all samples\n",
    "samples = [create_instruction_sample(row) for _, row in df.iterrows()]\n",
    "\n",
    "print(f\"Created {len(samples)} instruction samples\")\n",
    "print(f\"\\nExample sample:\")\n",
    "print(json.dumps(samples[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 240\n",
      "Validation samples: 27\n"
     ]
    }
   ],
   "source": [
    "# Split into train/validation sets (90/10 split)\n",
    "train_samples, val_samples = train_test_split(\n",
    "    samples, \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training data saved to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/train_instruct.jsonl\n",
      "✓ Validation data saved to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/val_instruct.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Save to JSONL format\n",
    "train_path = DATA_DIR / \"train_instruct.jsonl\"\n",
    "val_path = DATA_DIR / \"val_instruct.jsonl\"\n",
    "\n",
    "# Write training data\n",
    "with open(train_path, 'w') as f:\n",
    "    for sample in train_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "# Write validation data\n",
    "with open(val_path, 'w') as f:\n",
    "    for sample in val_samples:\n",
    "        f.write(json.dumps(sample) + '\\n')\n",
    "\n",
    "print(f\"✓ Training data saved to: {train_path}\")\n",
    "print(f\"✓ Validation data saved to: {val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Mistral-7B-Instruct-v0.3 Model\n",
    "\n",
    "Download the base model for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Mistral-7B-Instruct-v0.3...\n",
      "--2025-12-09 11:47:19--  https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar\n",
      "Resolving models.mistralcdn.com (models.mistralcdn.com)... 172.67.70.68, 104.26.6.117, 104.26.7.117, ...\n",
      "Connecting to models.mistralcdn.com (models.mistralcdn.com)|172.67.70.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14496675840 (14G) [application/x-tar]\n",
      "Saving to: ‘/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3.tar’\n",
      "\n",
      "/home/scumpia-mrl/D 100%[===================>]  13.50G  21.7MB/s    in 9m 1s   \n",
      "\n",
      "2025-12-09 11:56:21 (25.5 MB/s) - ‘/home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3.tar’ saved [14496675840/14496675840]\n",
      "\n",
      "Extracting model...\n",
      "✓ Model extracted to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3\n",
      "\n",
      "Verify checksum with: md5sum /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3.tar\n",
      "Expected: 80b71fcb6416085bcb4efad86dfb4d52\n"
     ]
    }
   ],
   "source": [
    "# Download Mistral-7B-Instruct-v0.3\n",
    "model_url = \"https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar\"\n",
    "model_tar = MODEL_DIR / \"mistral-7B-Instruct-v0.3.tar\"\n",
    "model_extract_dir = MODEL_DIR / \"mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "if not model_extract_dir.exists():\n",
    "    print(\"Downloading Mistral-7B-Instruct-v0.3...\")\n",
    "    !wget -O {model_tar} {model_url}\n",
    "    \n",
    "    print(\"Extracting model...\")\n",
    "    !tar -xf {model_tar} -C {MODEL_DIR}\n",
    "    \n",
    "    print(f\"✓ Model extracted to: {model_extract_dir}\")\n",
    "else:\n",
    "    print(f\"✓ Model already exists at: {model_extract_dir}\")\n",
    "\n",
    "# Verify checksum (optional)\n",
    "expected_checksum = \"80b71fcb6416085bcb4efad86dfb4d52\"\n",
    "print(f\"\\nVerify checksum with: md5sum {model_tar}\")\n",
    "print(f\"Expected: {expected_checksum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration saved to: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/train_config.yaml\n",
      "\n",
      "Configuration:\n",
      "data:\n",
      "  instruct_data: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/train_instruct.jsonl\n",
      "  data: ''\n",
      "  eval_instruct_data: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/val_instruct.jsonl\n",
      "model_id_or_path: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3\n",
      "lora:\n",
      "  rank: 64\n",
      "seq_len: 16384\n",
      "batch_size: 8\n",
      "max_steps: 500\n",
      "optim:\n",
      "  lr: 6.0e-05\n",
      "  weight_decay: 0.1\n",
      "  pct_start: 0.05\n",
      "seed: 42\n",
      "log_freq: 10\n",
      "eval_freq: 100\n",
      "no_eval: false\n",
      "ckpt_freq: 100\n",
      "save_adapters: true\n",
      "run_dir: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/output/run_001\n",
      "wandb:\n",
      "  project: mistral-7b-engineering-qa\n",
      "  run_name: engineering-docs-lora-2xRTX5000-33GB\n",
      "  key: ''\n",
      "  offline: true\n",
      "\n",
      "\n",
      "⚡ Optimized for 2x RTX 5000 Ada (33GB VRAM each):\n",
      "  • 16K context window for longer documents\n",
      "  • Batch size 8 (4 per GPU) for faster training\n",
      "  • Expected memory usage: ~20-24GB per GPU\n"
     ]
    }
   ],
   "source": [
    "# Training configuration for engineering document Q&A (optimized for 2x RTX 5000 Ada - 33GB each)\n",
    "config = {\n",
    "    # Data paths\n",
    "    \"data\": {\n",
    "        \"instruct_data\": str(train_path),\n",
    "        \"data\": \"\",  # No pretraining data\n",
    "        \"eval_instruct_data\": str(val_path)\n",
    "    },\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model_id_or_path\": str(model_extract_dir),\n",
    "    \"lora\": {\n",
    "        \"rank\": 64  # LoRA rank (can increase to 128 with 33GB VRAM)\n",
    "    },\n",
    "    \n",
    "    # Training hyperparameters (optimized for 2x RTX 5000 Ada - 33GB VRAM each)\n",
    "    \"seq_len\": 16384,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_steps\": 500,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    \"optim\": {\n",
    "        \"lr\": 6e-5,  # Learning rate\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"pct_start\": 0.05\n",
    "    },\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    \"seed\": 42,\n",
    "    \"log_freq\": 10,\n",
    "    \"eval_freq\": 100,\n",
    "    \"no_eval\": False,\n",
    "    \"ckpt_freq\": 100,\n",
    "    \n",
    "    # Save configuration\n",
    "    \"save_adapters\": True,  # Save only LoRA adapters (smaller size)\n",
    "    \"run_dir\": str(OUTPUT_DIR / \"run_001\"),\n",
    "    \n",
    "    # Weights & Biases (optional)\n",
    "    \"wandb\": {\n",
    "        \"project\": \"mistral-7b-engineering-qa\",\n",
    "        \"run_name\": \"engineering-docs-lora-2xRTX5000-33GB\",\n",
    "        \"key\": \"\",  # Add your W&B API key\n",
    "        \"offline\": True  # Set to False if using W&B\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = BASE_DIR / \"train_config.yaml\"\n",
    "\n",
    "import yaml\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✓ Configuration saved to: {config_path}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(yaml.dump(config, default_flow_style=False, sort_keys=False))\n",
    "print(f\"\\n⚡ Optimized for 2x RTX 5000 Ada (33GB VRAM each):\")\n",
    "print(f\"  • 16K context window for longer documents\")\n",
    "print(f\"  • Batch size 8 (4 per GPU) for faster training\")\n",
    "print(f\"  • Expected memory usage: ~20-24GB per GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOMMENDED: Multi-GPU training for 2x RTX 5000 Ada\n",
    "import subprocess\n",
    "\n",
    "# Change to mistral-finetune directory\n",
    "os.chdir(FINETUNE_DIR)\n",
    "\n",
    "# Run training with 2 GPUs\n",
    "cmd = [\n",
    "    \"torchrun\",\n",
    "    \"--nproc_per_node=2\",  # 2 GPUs\n",
    "    \"--master_port=29500\",\n",
    "    \"train.py\",\n",
    "    str(config_path)\n",
    "]\n",
    "\n",
    "print(\"Starting fine-tuning with 2x RTX 5000 Ada GPUs...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run training (this will take time depending on your GPU)\n",
    "subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitor Training\n",
    "\n",
    "Training logs and checkpoints will be saved to the `run_dir` specified in config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check training output\n",
    "run_dir = OUTPUT_DIR / \"run_001\"\n",
    "\n",
    "if run_dir.exists():\n",
    "    print(f\"Training artifacts in: {run_dir}\")\n",
    "    print(f\"\\nDirectory contents:\")\n",
    "    !ls -lh {run_dir}\n",
    "else:\n",
    "    print(f\"Training not started yet. Run the training cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training completes, you can load and test the model\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Paths\n",
    "base_model_path = model_extract_dir\n",
    "adapter_path = run_dir / \"checkpoints\" / \"checkpoint_500\"  # Final checkpoint\n",
    "\n",
    "# Load base model and tokenizer\n",
    "print(\"Loading base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(\"Loading LoRA adapter...\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fine-tuned model\n",
    "def generate_answer(question, max_length=512):\n",
    "    \"\"\"\n",
    "    Generate answer for a given question.\n",
    "    \"\"\"\n",
    "    system_prompt = (\n",
    "        \"You are a technical assistant specialized in commercial refrigeration equipment. \"\n",
    "        \"Provide accurate, concise answers based on equipment manuals and documentation.\"\n",
    "    )\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = f\"<s>[INST] {system_prompt}\\n\\n{question} [/INST]\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract answer (remove prompt)\n",
    "    answer = response.split('[/INST]')[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test with sample questions\n",
    "test_questions = [\n",
    "    \"What is the maximum defrost duration for an Ascend Freezer?\",\n",
    "    \"Can I use an extension cord for my Arctic Air Chef Base Refrigerator?\",\n",
    "    \"What should I do if the compressor is running too long?\"\n",
    "]\n",
    "\n",
    "print(\"Testing fine-tuned model:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\nQuestion {i}: {question}\")\n",
    "    print(f\"\\nAnswer: {generate_answer(question)}\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load validation samples\n",
    "val_df = df.iloc[-len(val_samples):].reset_index(drop=True)\n",
    "\n",
    "print(f\"Evaluating on {len(val_df)} validation samples...\\n\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for _, row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    question = row['input_query']\n",
    "    prediction = generate_answer(question)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Add to dataframe\n",
    "val_df['predicted_answer'] = predictions\n",
    "\n",
    "# Save results\n",
    "results_path = OUTPUT_DIR / \"validation_results.csv\"\n",
    "val_df.to_csv(results_path, index=False)\n",
    "print(f\"\\n✓ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Final Model (Optional)\n",
    "\n",
    "Merge LoRA adapters with base model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights with base model\n",
    "merged_model_path = OUTPUT_DIR / \"merged_model\"\n",
    "merged_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Merging LoRA adapter with base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(\"Saving merged model...\")\n",
    "merged_model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "\n",
    "print(f\"✓ Merged model saved to: {merged_model_path}\")\n",
    "print(f\"\\nModel size:\")\n",
    "!du -sh {merged_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. ✓ Data preparation: CSV → JSONL format for Mistral instruct training\n",
    "2. ✓ Model download: Mistral-7B-Instruct-v0.3\n",
    "3. ✓ Configuration: Optimized LoRA training setup\n",
    "4. ✓ Training: Single/multi-GPU fine-tuning with mistral-finetune\n",
    "5. ✓ Evaluation: Load and test fine-tuned model\n",
    "6. ✓ Export: Merge and save final model\n",
    "\n",
    "## Next Steps\n",
    "- Adjust hyperparameters (learning rate, batch size, max_steps) based on performance\n",
    "- Experiment with different LoRA ranks (32, 64, 128)\n",
    "- Add more training data for better domain adaptation\n",
    "- Implement quantization (4-bit/8-bit) for deployment efficiency\n",
    "- Deploy using vLLM or TGI for production serving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral-finetune (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
