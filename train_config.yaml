data:
  instruct_data: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/train_instruct.jsonl
  data: ''
  eval_instruct_data: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/data/val_instruct.jsonl
model_id_or_path: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/models/mistral-7B-Instruct-v0.3
lora:
  rank: 64  # LoRA rank - higher = more parameters but more memory
seq_len: 4096  # Sequence length - optimized for single GPU (31GB VRAM)
batch_size: 1  # Batch size - set to 1 for single GPU to avoid OOM
max_steps: 500  # Total training steps
optim:
  lr: 6.0e-05
  weight_decay: 0.1
  pct_start: 0.05
seed: 42
log_freq: 10
eval_freq: 100
no_eval: false
ckpt_freq: 100
save_adapters: true
run_dir: /home/scumpia-mrl/Desktop/Sujit/Projects/mistral-7B-instruct-finetune/output/run_001
wandb:
  project: mistral-7b-engineering-qa
  run_name: engineering-docs-lora-2xRTX5000-33GB
  key: 'c9c33d9ef9f0bfe8c68d5ff7a0e28a7333d0a71b'
  offline: false
